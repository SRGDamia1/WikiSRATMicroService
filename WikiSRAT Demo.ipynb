{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API docs for the WikiSRATMicroService\n",
    "##### This service will not work properly for the pollution assessment\n",
    "\n",
    "#### The Current WikiSRAT microservice is a local hotspot tool and is fundamentally different from the goal of the pollution assessment, which is to analyze watershed loads. The microservice does not work in a way to pass loads between huc12s, and so to complete this deliverable ANS will re-create SRAT using an updated MMW loading table and the updated attenuation methodology from the microservice. To update the service or create a new API is not feasable within the timeline of the pollution assessment project (end of September 2021). \n",
    "\n",
    "#### SRAT code is all in SQL, run from this notebook. This code is within the GitHub repository. The result of this will be a table of NHDplus catchments. \n",
    "\n",
    "1) Distribute the HUC12 loading table to the NHDplus catchments (re-allocation) based on the LULC proportion tables and point sources that we have.\n",
    "- i.e. Take last version of SRAT code and point to new loading table, update where necessary.\n",
    "\n",
    "2) Use the new routing routing to route loads down using the finish time of the NHDplus catchments (guarenteed topologically correct). Continue to sum loads as we move downstream. Load given + Load then attentuate and pass on (run for each NHD catchment). Reduction factors are changed and based on shedareadrainlake.\n",
    "- i.e. Change out attenuation routine to be align with new method by Barry.\n",
    "\n",
    "3) Calculate the stream concentrations from the watershed loads using mean annual flow.\n",
    "\n",
    "4) Have Barry review the results to make sure that we are using the correct values for shedareadrainlake coefficients.\n",
    "\n",
    "###### 2021/07/21 - Completion of scope of work to take take 1 - 2 months (goal is end of August)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import os\n",
    "import psycopg2\n",
    "from StringParser import StringParser\n",
    "from DatabaseAdapter import DatabaseAdapter\n",
    "from DatabaseFormatter import DatabaseFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# RAPLACE THIS WITH ACCESS INFORMATION\n",
    "_database = ''\n",
    "_host = ''\n",
    "_port = ''\n",
    "_user = ''\n",
    "_password = ''\n",
    "\n",
    "\n",
    "def respond(err, res=None):\n",
    "    return {\n",
    "        'statusCode': '400' if err else '200',\n",
    "        'body': err.args[0] if err else json.dumps(res),\n",
    "        'headers': {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Access-Control-Allow-Origin': '*'\n",
    "        },\n",
    "    }\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        data = StringParser.parse(event['body'])\n",
    "        db = DatabaseAdapter(_database, _user, _host, _port, _password)\n",
    "        input_array = DatabaseAdapter.python_to_array(data)\n",
    "        return respond(None, db.run_model(input_array))\n",
    "    except AttributeError as e:\n",
    "        return respond(e)\n",
    "\n",
    "_PG_Connection = psycopg2.connect(\n",
    "                host=_host,\n",
    "                database=_database,\n",
    "                user=_user,\n",
    "                password=_password,\n",
    "                port= _port)\n",
    "\n",
    "_PG_Connection.set_isolation_level(0)\n",
    "_cur = _PG_Connection.cursor()\n",
    "_cur.execute(\"select * from databmpapi.drb_loads_raw order by huc12;\")  \n",
    "# _cur.execute(\"select * from databmpapi.drb_loads_raw where huc12 in ('020402030902', '020402030901');\")  \n",
    "\n",
    "_dbdata = _cur.fetchall()\n",
    "print(len(_dbdata))\n",
    "\n",
    "_body = DatabaseFormatter.parse(_dbdata)\n",
    "# _body = '[{\"huc12\": \"020402010101\", \"tpload_hp\": 10, \"tpload_crop\": 10, \"tpload_wooded\": 10, \"tpload_open\": 10, \"tpload_barren\": 10, \"tpload_ldm\": 10, \"tpload_mdm\": 10, \"tpload_hdm\": 10, \"tpload_wetland\": 10, \"tpload_farman\": 10, \"tpload_tiledrain\": 10, \"tpload_streambank\": 10, \"tpload_subsurface\": 10, \"tpload_pointsource\": 10, \"tpload_septics\": 10, \"tnload_hp\": 10, \"tnload_crop\": 10, \"tnload_wooded\": 10, \"tnload_open\": 10, \"tnload_barren\": 10, \"tnload_ldm\": 10, \"tnload_mdm\": 10, \"tnload_hdm\": 10, \"tnload_wetland\": 10, \"tnload_farman\": 10, \"tnload_tiledrain\": 10, \"tnload_streambank\": 10, \"tnload_subsurface\": 10, \"tnload_pointsource\": 10, \"tnload_septics\": 10, \"tssload_hp\": 10, \"tssload_crop\": 10, \"tssload_wooded\": 10, \"tssload_open\": 10, \"tssload_barren\": 10, \"tssload_ldm\": 10, \"tssload_mdm\": 10, \"tssload_hdm\": 10, \"tssload_wetland\": 10, \"tssload_tiledrain\": 10, \"tssload_streambank\": 10}, {\"huc12\": \"020402010102\", \"tpload_hp\": 10, \"tpload_crop\": 10, \"tpload_wooded\": 10, \"tpload_open\": 10, \"tpload_barren\": 10, \"tpload_ldm\": 10, \"tpload_mdm\": 10, \"tpload_hdm\": 10, \"tpload_wetland\": 10, \"tpload_farman\": 10, \"tpload_tiledrain\": 10, \"tpload_streambank\": 10, \"tpload_subsurface\": 10, \"tpload_pointsource\": 10, \"tpload_septics\": 10, \"tnload_hp\": 10, \"tnload_crop\": 10, \"tnload_wooded\": 10, \"tnload_open\": 10, \"tnload_barren\": 10, \"tnload_ldm\": 10, \"tnload_mdm\": 10, \"tnload_hdm\": 10, \"tnload_wetland\": 10, \"tnload_farman\": 10, \"tnload_tiledrain\": 10, \"tnload_streambank\": 10, \"tnload_subsurface\": 10, \"tnload_pointsource\": 10, \"tnload_septics\": 10, \"tssload_hp\": 10, \"tssload_crop\": 10, \"tssload_wooded\": 10, \"tssload_open\": 10, \"tssload_barren\": 10, \"tssload_ldm\": 10, \"tssload_mdm\": 10, \"tssload_hdm\": 10, \"tssload_wetland\": 10, \"tssload_tiledrain\": 10, \"tssload_streambank\": 10}, {\"huc12\": \"020402010103\", \"tpload_hp\": 10, \"tpload_crop\": 10, \"tpload_wooded\": 10, \"tpload_open\": 10, \"tpload_barren\": 10, \"tpload_ldm\": 10, \"tpload_mdm\": 10, \"tpload_hdm\": 10, \"tpload_wetland\": 10, \"tpload_farman\": 10, \"tpload_tiledrain\": 10, \"tpload_streambank\": 10, \"tpload_subsurface\": 10, \"tpload_pointsource\": 10, \"tpload_septics\": 10, \"tnload_hp\": 10, \"tnload_crop\": 10, \"tnload_wooded\": 10, \"tnload_open\": 10, \"tnload_barren\": 10, \"tnload_ldm\": 10, \"tnload_mdm\": 10, \"tnload_hdm\": 10, \"tnload_wetland\": 10, \"tnload_farman\": 10, \"tnload_tiledrain\": 10, \"tnload_streambank\": 10, \"tnload_subsurface\": 10, \"tnload_pointsource\": 10, \"tnload_septics\": 10, \"tssload_hp\": 10, \"tssload_crop\": 10, \"tssload_wooded\": 10, \"tssload_open\": 10, \"tssload_barren\": 10, \"tssload_ldm\": 10, \"tssload_mdm\": 10, \"tssload_hdm\": 10, \"tssload_wetland\": 10, \"tssload_tiledrain\": 10, \"tssload_streambank\": 10}]'\n",
    "\n",
    "_r = dict(lambda_handler({\"body\": _body},None))\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict(json.loads(_r['body']))['huc12s']['020402010101']['catchments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE RESULTS INTO A DATABASE FOR REVIEW, CONSULT MSC94@DREXEL.EDU FOR MORE INFORMATION (REQUIRES HIGHER PERMISSION)\n",
    "\n",
    "len(dict(json.loads(_r['body']))['huc12s'])\n",
    "_nhdloads = dict(json.loads(_r['body']))['huc12s']\n",
    "for huc12s, huc12 in _nhdloads.items():\n",
    "    for comid in _nhdloads[huc12s]['catchments']:\n",
    "        update_arr = [int(_nhdloads[huc12s]['catchments'][comid]['comid']),\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tploadrate_total'],\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tploadate_conc'],\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tnloadrate_total'],\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tnloadrate_conc'],\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tssloadrate_total'],\n",
    "                      _nhdloads[huc12s]['catchments'][comid]['tssloadrate_conc']]\n",
    "        update_arr = [x if x != None else -9999 for x in update_arr]\n",
    "        _PG_Connection.set_isolation_level(0)\n",
    "        _cur = _PG_Connection.cursor()\n",
    "        _cur.execute(\"insert into wikiwtershed.z_testoutput values ({},{},{},{},{},{},{})\"\n",
    "                     \";\".format(update_arr[0],update_arr[1],update_arr[2],update_arr[3],update_arr[4],update_arr[5],update_arr[6])) \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WikiSRAT",
   "language": "python",
   "name": "wikisrat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
