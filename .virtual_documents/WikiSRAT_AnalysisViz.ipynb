# packages for data requests
import requests
import pandas as pd
from requests.auth import HTTPBasicAuth
import json
import os
import psycopg2

from pathlib import Path

# packages for viz 
import matplotlib
import matplotlib.pyplot as plt
# from pynhd import NLDI, WaterData, NHDPlusHR
# import pynhd as nhd
# import spatialpandas as spd
# import spatialpandas.io
# import pygeos
import geopandas as gpd
import plotly.express as px

import contextily as ctx
import numpy as np

# import holoviews as hv
# import datashader as ds
# import geoviews as gv
# import geoviews.feature as gf
# from geoviews import opts
# from cartopy import crs 
# import geoviews.tile_sources as gts
# gv.extension('bokeh', 'matplotlib')
# import hvplot.pandas



print("Geopandas: ", gpd.__version__)
# print("spatialpandas: ", spd.__version__)
# print("datashader: ", ds.__version__)
# print("pygeos: ", pygeos.__version__)


# Find current working directory
Path.cwd()


# Use relative path - will work for anybody in this directory / cloning the github
data_folder    = Path('data/')


get_ipython().run_cell_magic("time", "", """# read data from parquet files
base_catch_gdf = gpd.read_parquet(data_folder /'base_df_catch.parquet')
base_reach_gdf = gpd.read_parquet(data_folder /'base_df_reach.parquet')

rest_catch_gdf = gpd.read_parquet(data_folder /'rest_df_catch.parquet')
rest_reach_gdf = gpd.read_parquet(data_folder /'rest_df_reach.parquet')

point_src_gdf = gpd.read_parquet(data_folder /'point_source_df.parquet')

proj_prot_gdf = gpd.read_parquet(data_folder /'prot_proj_df.parquet')
proj_rest_gdf = gpd.read_parquet(data_folder /'rest_proj_df.parquet')

cluster_gdf   = gpd.read_parquet(data_folder /'cluster_df.parquet')   

mmw_huc12_loads_df = pd.read_parquet(data_folder /'mmw_huc12_loads_df.parquet')""")


focus


cluster_gdf   = gpd.read_parquet(data_folder /'cluster_df.parquet')


# Confirm files are read properly
base_catch_gdf.info()


base_catch_gdf.huc12.unique


rest_catch_gdf.info()


base_reach_gdf.info()


rest_reach_gdf.info()


proj_rest_gdf.info()


# Threshold/Target Values for Acceptable Water Quality

# Catchment Target Load Rate (kg/ha)
tn_loadrate_target  = 17.07  # Includes Organic N
tp_loadrate_target  = 0.31
tss_loadrate_target = 923.80

# Reach Target Concenctration (mg/l)
tn_conc_target  = 4.73  # Includes Organic N
tp_conc_target  = 0.09
tss_conc_target = 237.30

# Minimum Values, to avoid negative numbers and errors with LOG normalized plots
# = Targets / 100


# Create a dictionary of these Targets, to use later for iterating functions

targets = {'tn':  {'loadrate_target':tn_loadrate_target,
                   'conc_target': tn_conc_target},
           'tp':  {'loadrate_target':tp_loadrate_target,
                   'conc_target': tp_conc_target},
           'tss': {'loadrate_target':tss_loadrate_target,
                   'conc_target': tss_conc_target}
          }


targets['tp']


targets['tp']['conc_target']


# Cluster `geom` imported direclty from ANS in `WikiSRAT_Demo.ipynb` notebook
cluster_gdf.plot()


get_ipython().run_cell_magic("time", "", """# Create Focus Areas GeoDataFrame

focusarea_gdf = base_catch_gdf.dissolve(by='fa_name')""")


# This dataframe was created by dissolving geometries, so numerical field are not correct.

focusarea_gdf.info()


# Check CRS, which appears preserved in Parquet file metadata.
base_catch_gdf.crs


get_ipython().run_cell_magic("time", "", """# Test method to reproject CRS to 3857, which is useful for visualization
base_catch_gdf.to_crs(epsg=3857, inplace=True)""")


# Check CRS
base_catch_gdf.crs


get_ipython().run_cell_magic("time", "", """# Automate method to reproject CRS to 3857, for all GeoDataFrames
gdfs = [base_catch_gdf, base_reach_gdf, rest_catch_gdf, rest_reach_gdf, point_src_gdf, proj_prot_gdf, proj_rest_gdf, cluster_gdf, focusarea_gdf]

for gdf in gdfs:
    gdf.to_crs(epsg=3857, inplace=True)""")


# Confirm files are read properly
base_catch_gdf.info()


# check that it worked
point_src_gdf.crs


cluster_gdf.crs


focusarea_gdf.crs


# Example selecting by comid, which is the row index
base_reach_gdf.loc[4648450]


base_catch_gdf.loc[4648450]


# Annual Areal Loading Rate, for Total Phosphorus (kg/ha)
base_catch_gdf.loc[4648450].tp_load / base_catch_gdf.loc[4648450].catchment_hectares 


# Basic selection by column name
var = 'tp_load'
rest_catch_gdf[var]


# Select Series by attribute
base_catch_gdf.tp_load


# Calculate statistics for a data series
base_catch_gdf.tp_load.min()


base_catch_gdf.tp_load.max()


# Calculate statistics for a data series, filtered by a categorical column.
base_reach_gdf[base_reach_gdf.cluster=='drb'].tp_conc.mean()


# # Explict use of Pandas/GeoPandas functionality

# base_catch_gdf['tn_loadrate']  = base_catch_gdf.tn_load /  base_catch_gdf.catchment_hectares 
# base_catch_gdf['tp_loadrate']  = base_catch_gdf.tp_load /  base_catch_gdf.catchment_hectares 
# base_catch_gdf['tss_loadrate'] = base_catch_gdf.tss_load / base_catch_gdf.catchment_hectares 


# Define a function to loop through all the pollutants in a data frame
# for speed and ease that can also be applied to restoration dataframes.

def CalcLoadRate(df):
    for pollutant in ['tn','tp','tss']:
        df['%s_loadrate' % pollutant] = df['%s_load' % pollutant] / df.catchment_hectares
    return df


# Apply to the base model run catchments
base_catch_gdf = CalcLoadRate(base_catch_gdf)


# Confirm values in Model My Watershed, from https://modelmywatershed.org/project/36183/
base_catch_gdf.loc[4648450].tp_loadrate


base_catch_gdf.loc[4648450].tn_loadrate


base_catch_gdf.loc[4648450].tss_loadrate


base_catch_gdf.info()


# Apply to the restoration model run catchments
rest_catch_gdf = CalcLoadRate(rest_catch_gdf)


rest_catch_gdf.info()


diff_df = base_catch_gdf.tp_load == rest_catch_gdf.tp_load
diff_df.value_counts()


diff_df = base_reach_gdf.tp_conc == rest_reach_gdf.tp_conc
diff_df.value_counts()


# Drop NaN values, which show up as False in comparisions
diff_df = base_reach_gdf.tp_conc.dropna() == rest_reach_gdf.tp_conc.dropna()
diff_df.value_counts()


get_ipython().run_cell_magic("time", "", """fig, (ax1, ax2) = plt.subplots(1,2)
base_reach_gdf.to_crs(epsg=3857).plot(lw=1, ax=ax1)
base_catch_gdf.to_crs(epsg=3857).plot(lw=0.1, ax=ax2)
fig.set_size_inches(12,12)
ax1.set_title("Reaches")
ax2.set_title("Catchments")
for ax in [ax1, ax2]:
    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, 
                    crs='EPSG:3857', zoom=7)# crs=base_reach_gdf.crs.to_string(), zoom=7)
    ax.set_xlim(-8.56 * 10**6, -8.17 * 10**6)
    ax.set_ylim(4.65* 10**6, 5.26 * 10**6)
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)

plt.show()""")


# This function is not used in `PlotMaps` function below
def CalcMinMax(reach_df, catch_df, var_reach, var_catch):
    vmin = min(reach_df[reach_df[var_reach] > 0][var_reach].min(), catch_df[catch_df[var_catch] > 0][var_catch].min())
    vmax = max(reach_df[var_reach].max(), catch_df[var_catch].max())
    return vmin, vmax


def FormatAxes(ax, bounds=[-8.56 * 10**6,  -8.17 * 10**6, 4.65* 10**6, 5.26 * 10**6]):
    ax.set_xlim(bounds[0], bounds[1])
    ax.set_ylim(bounds[2], bounds[3])
    ax.axes.xaxis.set_visible(False)
    ax.axes.yaxis.set_visible(False)


get_ipython().run_cell_magic("time", "", """# Plot colors by values of selected variable (`var`)

var = 'maflowv'  # Mean Annual Flow Volumentric ('maflowv')

vmin, vmax = CalcMinMax(base_reach_gdf, base_catch_gdf, var, var)

fig, (ax1, ax2) = plt.subplots(1,2)
# ax3 = fig.add_axes([0.85, 0.1, 0.1, 0.8])

r = base_reach_gdf.plot(column=var,lw=1, ax=ax1, norm=matplotlib.colors.LogNorm(vmin, vmax), cmap='RdYlGn_r') 
c = base_catch_gdf.plot(column=var, lw=0.1, ax=ax2, norm=matplotlib.colors.LogNorm(vmin, vmax), cmap='RdYlGn_r')
fig.set_size_inches(12,12)
ax1.set_title(var + " for Reaches")
ax2.set_title(var + " for Catchments")

# add colorbar 
cax = fig.add_axes([0.95, 0.22, 0.04, 0.57]) # adjusts the position of the color bar: right position, bottom, width, top 
sm = plt.cm.ScalarMappable(cmap='RdYlGn_r',
                           norm=matplotlib.colors.LogNorm(vmin=vmin, vmax=vmax))
cbr = fig.colorbar(sm, cax=cax,)

for ax in [ax1, ax2]:
    FormatAxes(ax)
    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=base_catch_gdf.crs.to_string(), zoom=7)

# plt.colorbar(c, ax = ax3)
plt.show()""")


# https://stackoverflow.com/questions/48625475/python-shifted-logarithmic-colorbar-white-color-offset-to-center
# centers logscale colorbar around provided value 
from  matplotlib.colors import LogNorm

class MidPointLogNorm(LogNorm):
    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
        LogNorm.__init__(self,vmin=vmin, vmax=vmax, clip=clip)
        self.midpoint=midpoint
    def __call__(self, value, clip=None):
        # I'm ignoring masked values and all kinds of edge cases to make a
        # simple example...
        x, y = [np.log(self.vmin), np.log(self.midpoint), np.log(self.vmax)], [0, 0.5, 1]
        return np.ma.masked_array(np.interp(np.log(value), x, y))


def LatLonExtent(cluster_name):
    lats = []
    lons = []

    values = cluster_gdf[cluster_gdf.index==cluster_name].geom.bounds
    
    y_extent = (values.maxy - values.miny) 
    x_extent = (values.maxx - values.minx)
    
    y_extent = y_extent[0] 
    x_extent = x_extent[0]
    
    # add 5 percent cushion
    x_cushion = x_extent * 0.05
    y_cushion = y_extent * 0.05
    
    aspect = (5.26 * 10**6 - 4.65* 10**6)/ (8.56 * 10**6 - 8.17 * 10**6)
    base_aspect = (y_extent + y_cushion) / (x_extent + x_cushion)
    
    # print('Aspect = ', aspect)  # for debugging with Zoom

    
    if base_aspect > aspect:
        lat_max = values.maxy + y_cushion
        lat_min = values.miny - y_cushion
        
        x_tot = (y_extent + 2*y_cushion) / aspect 
        x_pad = (x_tot - x_extent) / 2
        
        lon_max = values.maxx + x_pad
        lon_min = values.minx - x_pad
        h_v = "vertical"
    
    elif base_aspect < aspect:
        lon_max = values.maxx + x_cushion
        lon_min = values.minx - x_cushion 
        
        y_tot = (x_extent + 2*x_cushion) * aspect
        y_pad = (y_tot - y_extent) / 2
        
        lat_max = values.maxy + y_pad
        lat_min = values.miny - y_pad
        
        h_v = "horizontal"
        
    else:
        lon_max = values.maxx + x_extent
        lon_min = values.minx - x_extent
        lat_max = values.maxy + y_extent
        lat_min = values.miny - y_extent
        
        h_v = "exact"
    
    
    area = x_extent*y_extent / 1000000000
    
    return lon_max[0], lon_min[0], lat_max[0], lat_min[0], area, h_v


base_reach_gdf.tss_conc.min()


get_ipython().run_cell_magic("time", "", """var_reach = 'tss_conc'
targ_reach = tss_conc_target

dp_reach = base_reach_gdf.loc[:,(var_reach, 'geom')]  # Avoids 'SettingWithCopyWarning'. See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy""")


get_ipython().run_cell_magic("time", "", """mask = dp_reach[var_reach] < targ_reach / 100""")


mask.value_counts()


get_ipython().run_cell_magic("time", "", """dp_reach.loc[mask,var_reach] = targ_reach / 100""")


dp_reach[var_reach].min()


dp_reach[var_reach].value_counts()


base_reach_gdf.info()


var_reach = 'tn_xyz'


var_reach.split('_')[0] + '_conc'


def PlotMaps(df_reach, df_catch, var_reach, var_catch, targ_reach, targ_catch, cl=None, fa=False, zoom=False, diff=False):
    # remove <0 values for plotting, setting to target/100
    dp_reach = df_reach.loc[:,(var_reach, 'geom')]  # Avoids 'SettingWithCopyWarning'. See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copydp_catch = df_catch[[var_catch, 'geom_catchment']].copy()  # Make explict copy, to avoid 'SettingWithCopyWarning'
    dp_catch = df_catch.loc[:,(var_catch, 'geom_catchment')]  # Avoids 'SettingWithCopyWarning'. 
    
    mask_reach = dp_reach[var_reach] < targ_reach / 100
    mask_catch = dp_catch[var_catch] < targ_catch / 100
    
    dp_reach.loc[mask_reach,var_reach] = targ_reach / 100
    dp_catch.loc[mask_catch,var_catch] = targ_catch / 100

    # initialize figure
    fig, (ax1, ax2) = plt.subplots(1,2)
    # ax3 = fig.add_axes([0.85, 0.1, 0.1, 0.8])
    
    #plot reach and catchment
    # keep min & max constant for all plots for each pollutant

    # Set midpoint lower if a difference calculation, 
    if diff == False:
        min_reach = dp_reach[var_reach].min()
        min_catch = dp_catch[var_catch].min()
        mid_reach = targ_reach
        mid_catch = targ_catch
        max_reach = dp_reach[var_reach].max()
        max_catch = dp_catch[var_catch].max()
    else:
        min_reach = targ_reach / 100
        min_catch = targ_catch / 100
        mid_reach = targ_reach / 30
        mid_catch = targ_catch / 30
        max_reach = df_reach[var_reach.split('_')[0] + '_conc'].max()
        max_catch = df_catch[var_catch.split('_')[0] + '_loadrate'].max()

    # normalize around target with MidPointLogNorm
    r = dp_reach.plot(column=var_reach, lw=1, ax=ax1,
                      norm= MidPointLogNorm(vmin=min_reach,
                                            vmax=max_reach, 
                                            midpoint=mid_reach),
                      cmap = 'RdYlGn_r')# matplotlib.colors.LogNorm(vmin, vmax), cmap='RdYlGn_r')
    c = dp_catch.plot(column=var_catch, lw=0.1, ax=ax2, 
                      norm= MidPointLogNorm(vmin=min_catch,
                                            vmax=max_catch, 
                                            midpoint=mid_catch),
                      cmap='RdYlGn_r')

    # plot cluster, if applicable
    if cl != None:
        # Display Cluster Name
        print('Cluster Name = ', cl)
        # plot cluster
        cl_reach = cluster_gdf[cluster_gdf.index == cl].plot(lw=1, ax=ax1, facecolor="none", edgecolor="black", zorder=10)
        cl_catch = cluster_gdf[cluster_gdf.index == cl].plot(lw=1, ax=ax2, facecolor="none", edgecolor="black")

    # plot focus areas within clusters
    if fa == True:
        fas = df_catch[df_catch.cluster == cl]['fa_name'].unique().dropna()
        fas_in_cluster = focusarea_gdf.loc[fas, :]
        fa_reach = fas_in_cluster.plot(lw=0.7, ax = ax1, facecolor="none", edgecolor="grey", zorder=10)
        fa_catch = fas_in_cluster.plot(lw=0.7, ax=ax2, facecolor = "none", edgecolor="grey")

    # set figure size 
    fig.set_size_inches(12,12)

    # zoom in to cluster if zoom = True 
    if zoom == True:
        if cl == None:
            print("No cluster entered!")
        else:
            lon_max, lon_min, lat_max, lat_min, area, h_v = LatLonExtent(cl)
            for ax in [ax1, ax2]:
                FormatAxes(ax, bounds=[lon_min, lon_max, lat_min, lat_max])
    else:
        for ax in [ax1, ax2]:
            FormatAxes(ax)

    # set axis titles
    ax1.set_title(var_reach + " (mg/L) for Reaches")
    ax2.set_title(var_catch + " (kg/ha) for Catchments")

    # add colorbar - catchment 
    cax = fig.add_axes([0.95, 0.18, 0.02, 0.64]) # adjusts the position of the color bar: right position, bottom, width, top 
    sm = plt.cm.ScalarMappable(cmap='RdYlGn_r', 
                               norm= MidPointLogNorm(vmin=min_catch,
                                                     vmax=max_catch, 
                                                     midpoint=mid_catch))
    cbr = fig.colorbar(sm, cax=cax,)
    cbr.ax.tick_params(labelsize=8)
    cbr.ax.minorticks_off()

    # add colorbar - reach
    cax2 = fig.add_axes([0.48, 0.18, 0.02, 0.64]) # adjusts the position of the color bar: right position, bottom, width, top 
    sm2 = plt.cm.ScalarMappable(cmap='RdYlGn_r',
                               norm=MidPointLogNorm(vmin=min_reach,
                                                    vmax=max_reach, 
                                                    midpoint=mid_reach))
    cbr2 = fig.colorbar(sm2, cax=cax2,)
    cbr2.ax.minorticks_off()
    cbr2.ax.tick_params(labelsize=8) 

    for ax in [ax1, ax2]:
        if zoom==False:
            ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=dp_reach.crs.to_string(), zoom=7)
        else:
            # change zoom of basemap based on coverage area
            if area < 7:
                ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=dp_reach.crs.to_string(), zoom=10)
            else:
                ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=dp_reach.crs.to_string(), zoom=9)

    # naming - #cluster_FA_ZOOM_varreach_varcatch.svg
    # can adjust this convention as desired 
    if cl == None:
        cl_name = ""
    else:
        cl_name = cl + "_"
    if fa==False:
        fa_name = ""
    else:
        fa_name = "FA_"
    if zoom==False:
        zoom_name = ""
    else:
        zoom_name = "Zoom_"

    fig.tight_layout(pad=5)
    plt.savefig('figs/%s%s%s%s_%s.svg' % (cl_name, fa_name, zoom_name, var_reach, var_catch)) # to automatically save - can adjust dpi, etch 
    plt.savefig('figs/%s%s%s%s_%s.png' % (cl_name, fa_name, zoom_name, var_reach, var_catch)) # to automatically save - can adjust dpi, etch 
    plt.show()


# Confirm files are read properly
base_catch_gdf.info()


get_ipython().run_cell_magic("time", "", """PlotMaps(base_reach_gdf, base_catch_gdf, 
        #  'tn_conc', 'tn_loadrate', tn_conc_target, tn_loadrate_target, 
         'tp_conc', 'tp_loadrate', tp_conc_target, tp_loadrate_target, 
        #  'tss_conc', 'tss_loadrate', tss_conc_target, tss_loadrate_target, 
        )""")


import warnings
warnings.filterwarnings('ignore', message='.*This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.*')


get_ipython().run_cell_magic("time", "", """for pollutant in ['tn', 'tp', 'tss']:
        PlotMaps(base_reach_gdf, base_catch_gdf, 
                 '%s_conc' % pollutant, 
                 '%s_loadrate' % pollutant, 
                 targets['%s' % pollutant]['conc_target'],
                 targets['%s' % pollutant]['loadrate_target'], 
                )""")


cluster_gdf.index.categories


cluster_gdf.plot()


print(tp_conc_target, tp_loadrate_target)


cluster_gdf.index[0]


get_ipython().run_cell_magic("time", "", """PlotMaps(base_reach_gdf, base_catch_gdf, 'tp_conc', 'tp_loadrate', tp_conc_target, tp_loadrate_target, cl=cluster_gdf.index[0])""")


PlotMaps(base_reach_gdf, base_catch_gdf, 
         'tp_conc', 'tp_loadrate', tp_conc_target, tp_loadrate_target, 
         cl=cluster_gdf.index[1], fa=True)


# horizontal
PlotMaps(base_reach_gdf, base_catch_gdf, 
         'tp_conc', 'tp_loadrate', tp_conc_target, tp_loadrate_target, 
         cl=cluster_gdf.index[1], fa=True, zoom=True)


# vertical
PlotMaps(base_reach_gdf, base_catch_gdf, 'tp_conc', 'tp_loadrate', 
         tp_conc_target, tp_loadrate_target, cl=cluster_gdf.index[2], fa=True, zoom=True)


def PlotZoom(df_reach, df_catch, var_reach, var_catch, targ_reach, targ_catch, cl=None):
    # initialize figure
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)
    fas = df_catch[df_catch.cluster == cl]['fa_name'].unique().dropna()
    fas_in_cluster = focusarea_gdf.loc[fas, :]
    # plot cluster
    for ax in [ax1, ax2, ax3, ax4]:
        cluster_gdf[cluster_gdf.index == cl].plot(lw=1, ax=ax, facecolor="none", edgecolor="black", zorder=10)
        fas_in_cluster.plot(lw=0.7, ax = ax, facecolor="none", edgecolor="grey", zorder=10)
        fig.set_size_inches(12,12)

    lon_max, lon_min, lat_max, lat_min, area, h_v  = LatLonExtent(cl)
    if h_v == "vertical":
        for ax in [ax1, ax2]:
            FormatAxes(ax, bounds=[lon_min, lon_max, lat_min + (lat_max - lat_min) / 2, lat_max])
        for ax in [ax3, ax4]:
            FormatAxes(ax, bounds=[lon_min, lon_max, lat_min, lat_min + (lat_max - lat_min) / 2])    
        plot_order = [ax1, ax2, ax3, ax4]
    elif h_v == 'horizontal':
        for ax in [ax1, ax3]:
            FormatAxes(ax, bounds=[lon_min, lon_min - (lon_min - lon_max) / 2, lat_min, lat_max])
        for ax in [ax2, ax4]:
            FormatAxes(ax, bounds=[lon_min - (lon_min - lon_max) / 2, lon_max, lat_min, lat_max])
        plot_order = [ax1, ax3, ax2, ax4]

    # plot reach and catchment
    # normalize around target with MidPointLogNorm
    r1 = df_reach.plot(column=var_reach, lw=1, ax=plot_order[0],
                          norm= MidPointLogNorm(vmin=df_reach[var_reach].min(),
                                                vmax=df_reach[var_reach].max(),
                                                midpoint=targ_reach),
                          cmap = 'RdYlGn_r')
    c1 = df_catch.plot(column=var_catch, lw=0.1, ax=plot_order[1],
                          norm= MidPointLogNorm(vmin=df_catch[var_catch].min(),
                                                vmax=df_catch[var_catch].max(),
                                                midpoint=targ_catch),
                          cmap='RdYlGn_r')

    r2 = df_reach.plot(column=var_reach, lw=1, ax=plot_order[2],
                          norm= MidPointLogNorm(vmin=df_reach[var_reach].min(),
                                                vmax=df_reach[var_reach].max(),
                                                midpoint=targ_reach),
                          cmap = 'RdYlGn_r')
    c2 = df_catch.plot(column=var_catch, lw=0.1, ax=plot_order[3],
                          norm= MidPointLogNorm(vmin=df_catch[var_catch].min(),
                                                vmax=df_catch[var_catch].max(),
                                                midpoint=targ_catch),
                          cmap='RdYlGn_r')

    if h_v == "vertical":
        ax1.set_title(var_reach + " for Reaches \n %s %s Cluster" % ("Northern", cl))
        ax2.set_title(var_catch + " for Catchments \n %s %s Cluster" % ("Northern", cl))
        ax3.set_title(var_reach + " for Reaches \n %s %s Cluster" % ("Southern", cl))
        ax4.set_title(var_catch + " for Catchments \n %s %s Cluster" % ("Southern", cl))
    elif h_v == "horizontal":
        ax1.set_title("%s for Reaches \n %s %s Cluster" % (var_reach, "Eastern", cl))
        ax2.set_title("%s for Reaches \n %s %s Cluster" % (var_reach, "Western", cl))
        ax3.set_title("%s for Catchments \n %s %s Cluster" % (var_catch, "Eastern", cl))
        ax4.set_title("%s for Catchments \n %s %s Cluster" % (var_catch, "Western", cl))        

    for ax in [ax1, ax2, ax3, ax4]:
        if area < 7:
            ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=df_reach.crs.to_string(), zoom=10)
        else:
            ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=df_reach.crs.to_string(), zoom=9)
    plt.show()


PlotZoom(base_reach_gdf, base_catch_gdf, 'tp_conc', 'tp_loadrate', 
         tp_conc_target, tp_loadrate_target, cl=cluster_gdf.index[2])


proj_rest_gdf


proj_rest_gdf.project_status.value_counts()


proj_prot_gdf


proj_prot_gdf.info()


# Dissolve (Aggregate) by Protection Project
# Preselect colums to keep

proj_prot_projects_gdf = proj_prot_gdf.dissolve('project_na')

proj_prot_projects_gdf


proj_prot_projects_gdf.info()


# Save to CSV file
proj_prot_projects_gdf.iloc[:,1:].to_csv(data_folder /'proj_prot_projects.csv')


# # Explict use of Pandas/GeoPandas functionality

# # Create new columns
# base_catch_gdf['tn_loadrate_xs']  = base_catch_gdf.tn_loadrate  -  tn_loadrate_target
# base_catch_gdf['tp_loadrate_xs']  = base_catch_gdf.tp_loadrate  -  tp_loadrate_target
# base_catch_gdf['tss_loadrate_xs'] = base_catch_gdf.tss_loadrate -  tss_loadrate_target


# Define a function to loop through all the pollutants in a data frame
def CalcExcessLoadRate(df, targ_dict):
    for pollutant in ['tn', 'tp', 'tss']:
        df['%s_loadrate_xs' % pollutant] = df['%s_loadrate' % pollutant] - targ_dict[pollutant]['loadrate_target']
    return df


base_catch_gdf = CalcExcessLoadRate(base_catch_gdf, targets)


base_catch_gdf.info()


# Confirm calculation relative to Model My Watershed
base_catch_gdf.loc[4648450].tp_loadrate


base_catch_gdf.loc[4648450].tp_loadrate_xs


# Difference should equal target value
targets['tp']['loadrate_target']


base_catch_gdf.tp_loadrate_xs.min()


base_catch_gdf.tn_loadrate_xs.min()


base_catch_gdf.tss_loadrate_xs.min()


test = base_catch_gdf.tss_loadrate_xs.copy()
test.min()


tss_loadrate_target / 100


test[test < tss_loadrate_target / 100] = tss_loadrate_target / 100
test.min()


test.size


test.value_counts()


# # Explict use of Pandas/GeoPandas functionality

# # Create new columns
# base_gdf_proj['tn_conc_xs']  = base_gdf_proj.tn_conc  -  tn_conc_target
# base_gdf_proj['tp_conc_xs']  = base_gdf_proj.tp_conc  -  tp_conc_target
# base_gdf_proj['tss_conc_xs'] = base_gdf_proj.tss_conc -  tss_conc_target


# Define a function to loop through all the pollutants in a data frame
def CalcExcessConcs(df, targ_dict):
    for pollutant in ['tn', 'tp', 'tss']:
        df['%s_conc_xs' % pollutant] = df['%s_conc' % pollutant] - targ_dict[pollutant]['conc_target']
    return df


base_reach_gdf = CalcExcessConcs(base_reach_gdf, targets)


base_reach_gdf.info()


base_reach_gdf.loc[4648450].tp_conc


base_reach_gdf.loc[4648450].tp_conc_xs


targets['tp']['conc_target']


base_reach_gdf.tp_conc_xs.min()


base_reach_gdf.tn_conc_xs.min()


base_reach_gdf.tss_conc_xs.min()


# PlotMaps(base_reach_gdf, base_catch_gdf, 
#         #  'tn_conc_xs', 'tn_loadrate_xs', tn_conc_target, tn_loadrate_target, 
#         #  'tp_conc_xs', 'tp_loadrate_xs', tp_conc_target, tp_loadrate_target, 
#          'tss_conc_xs', 'tss_loadrate_xs', tss_conc_target, tss_loadrate_target, 
#          cl=None, fa=False, zoom=False, diff=True)


# Excess Pollution in DRB
for pollutant in ['tn', 'tp', 'tss']:
    PlotMaps(base_reach_gdf, base_catch_gdf, 
             '%s_conc_xs' % pollutant, '%s_loadrate_xs' % pollutant, 
             targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
             cl=None, fa=False, zoom=False, diff=True)


# Excess Pollution in a Cluster
PlotMaps(base_reach_gdf, base_catch_gdf, 
        #  'tn_conc_xs', 'tn_loadrate_xs', tn_conc_target, tn_loadrate_target, 
         'tp_conc_xs', 'tp_loadrate_xs', tp_conc_target, tp_loadrate_target, 
        #  'tss_conc_xs', 'tss_loadrate_xs', tss_conc_target, tss_loadrate_target, 
         cl='New Jersey Highlands', fa=True, zoom=True, diff=True)


point_src_gdf


point_src_gdf.info()


# Confirm values relative to Model My Watershed, using Analyze tab.
# Use Upper West Branch Brandywine Creek, HUC-12 (020402050202). https://modelmywatershed.org/project/36183/.

point_src_gdf.loc['PA0026859']


# Look at all point sources in this COMID
point_src_gdf[point_src_gdf['comid']==932040160]


# Many COMID's have more than one NPDES-permitted point source
point_src_gdf.comid.value_counts()


# Sum loads by COMID groups
# Non-summable dtypes (object, category, geometry) will be dropped automatically
temp_df = point_src_gdf.groupby('comid').sum()

# Other fields that should not be summed, such as lat/lon or concentrations, need to dropped explicitly
point_src_loads_comid_df = temp_df.drop(['ogc_fid',
                                         'latitude',
                                         'longitude',
                                         'avg_n_conc',
                                         'avgpconc',
                                        ], axis=1) 
point_src_loads_comid_df


# Confirm values relative to Model My Watershed, using Model tab.
# Use Upper West Branch Brandywine Creek, HUC-12 (020402050202). https://modelmywatershed.org/project/36183/.

# **These values are bigger than the MMW Model totals for COMID!**
point_src_loads_comid_df.loc[932040160]


## These values match the MMW totals for COMID!
base_catch_gdf.tn_load.loc[932040160]


## These values match the MMW totals for COMID!
base_catch_gdf.tp_load.loc[932040160]


# Sum loads by COMID groups
# Non-summable dtypes (object, category, geometry) will be dropped automatically
temp_df = point_src_gdf.groupby('huc12').sum()

# Other fields that should not be summed, such as lat/lon or concentrations, need to dropped explicitly
point_src_loads_huc12_df = temp_df.drop(['ogc_fid',
                                         'latitude',
                                         'longitude',
                                         'avg_n_conc',
                                         'avgpconc',
                                         'comid'
                                        ], axis=1) 
point_src_loads_huc12_df


point_src_loads_huc12_df.size


# Non-Attenuated Point Source loads
point_src_loads_huc12_df.kgn_yr['020402050202']


# Attenuated Point Source loads, which matches MMW when running HUC12 alone (different than in subbasin mode for P)
mmw_huc12_loads_df.tn_pt_source_kg['020402050202']


# Nitrogen attenuation fraction, not retained (i.e. attenuated load / raw load)
mmw_huc12_loads_df.tn_pt_source_kg['020402050202'] / point_src_loads_huc12_df.kgn_yr['020402050202']


# Phosphorus attenuation, fraction not retained
mmw_huc12_loads_df.tp_pt_source_kg['020402050202'] / point_src_loads_huc12_df.kgp_yr['020402050202']


# Calcuate attenuation fraction
point_src_loads_huc12_df['tn_load_ps_atn_frac'] = mmw_huc12_loads_df.tn_pt_source_kg / point_src_loads_huc12_df.kgn_yr
point_src_loads_huc12_df['tp_load_ps_atn_frac'] = mmw_huc12_loads_df.tp_pt_source_kg / point_src_loads_huc12_df.kgp_yr


point_src_loads_huc12_df


# # Explict use of Pandas/GeoPandas functionality
# This simple merge/join is possible, because both dataframes share the same index (comid)

base_catch_gdf['tn_load_ps'] = point_src_loads_comid_df.kgn_yr
base_catch_gdf['tp_load_ps'] = point_src_loads_comid_df.kgp_yr 


# Fill NaN with 0, because arithmetric with a NaN produces NaN!
base_catch_gdf[['tn_load_ps','tp_load_ps']] = base_catch_gdf[['tn_load_ps','tp_load_ps']].fillna(0)


base_catch_gdf[['tp_load','tp_load_ps']].loc[932040160]


base_catch_gdf[['tp_load','tp_load_ps']].loc[4648450]


base_catch_gdf.info()


# # Explict use of Pandas/GeoPandas functionality

base_catch_gdf['tn_loadrate_xsnps']  = ((base_catch_gdf.tn_load - base_catch_gdf.tn_load_ps
                                        )  / base_catch_gdf.catchment_hectares
                                       ) - tn_loadrate_target

base_catch_gdf['tp_loadrate_xsnps']  = ((base_catch_gdf.tp_load - base_catch_gdf.tp_load_ps
                                        )  / base_catch_gdf.catchment_hectares
                                       ) - tp_loadrate_target
# TSS doesn't come from NPDES point sources


base_catch_gdf.info()


base_catch_gdf[['tn_loadrate',
                'tn_loadrate_xs',
                'tn_loadrate_xsnps',
               ]].loc[4648450]


base_catch_gdf[['tp_loadrate',
                'tp_loadrate_xs',
                'tp_loadrate_xsnps',
               ]].loc[4648450]


base_catch_gdf[['tn_loadrate',
                'tn_loadrate_xs',
                'tn_loadrate_xsnps',
               ]].loc[932040160]


base_catch_gdf[['tp_loadrate',
                'tp_loadrate_xs',
                'tp_loadrate_xsnps',
               ]].loc[932040160]


# Excess NPS Pollution in DRB
for pollutant in ['tn', 'tp']:  # TSS does not have a point source
    PlotMaps(base_reach_gdf, base_catch_gdf, 
             '%s_conc_xs' % pollutant, '%s_loadrate_xsnps' % pollutant, 
             targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
             cl=None, fa=False, zoom=False, diff=True)


get_ipython().run_cell_magic("time", "", """# Excess NPS Pollution in a Cluster
for cluster in cluster_gdf.index.categories:
    for pollutant in ['tn','tp']:  # TSS does not have a point source
        PlotMaps(base_reach_gdf, base_catch_gdf, 
                 '%s_conc_xs' % pollutant, '%s_loadrate_xsnps' % pollutant, 
                 targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
                 cl=cluster, fa=True, zoom=True, diff=True)""")


get_ipython().run_cell_magic("time", "", """# Excess Pollution in a Cluster, for TSS
for cluster in cluster_gdf.index.categories:
    for pollutant in ['tss']:
        PlotMaps(base_reach_gdf, base_catch_gdf, 
                 '%s_conc_xs' % pollutant, '%s_loadrate_xs' % pollutant, 
                 targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
                 cl=cluster, fa=True, zoom=True, diff=True)""")


# Concentration reductions for Reaches
base_reach_gdf['tn_conc_red']  = base_reach_gdf.tn_conc  - rest_reach_gdf.tn_conc
base_reach_gdf['tp_conc_red']  = base_reach_gdf.tp_conc  - rest_reach_gdf.tp_conc
base_reach_gdf['tss_conc_red'] = base_reach_gdf.tss_conc - rest_reach_gdf.tss_conc


# Load Rate reductions for Catchments
base_catch_gdf['tn_loadrate_red']  = base_catch_gdf.tn_loadrate  - rest_catch_gdf.tn_loadrate
base_catch_gdf['tp_loadrate_red']  = base_catch_gdf.tp_loadrate  - rest_catch_gdf.tp_loadrate
base_catch_gdf['tss_loadrate_red'] = base_catch_gdf.tss_loadrate - rest_catch_gdf.tss_loadrate


base_catch_gdf.iloc[:,-3:].max()


proj_prot_gdf.info()


# Sum loads by COMID
# Preselect colums to keep
proj_prot_loads_comid_df = proj_prot_gdf.iloc[:, [3, 16, 31, 28, 25, ]
                             ].groupby('comid').sum()

proj_prot_loads_comid_df.info()


# Pollution Avoided
# Convert to kg/ha, where `1 kg/ha = 0.89218 lb/ac`, http://www.kylesconverter.com/area-density/kilograms-per-hectare-to-pounds-per-acre

base_catch_gdf['tn_load_avoid']  = proj_prot_loads_comid_df.parcel_tnload_lbyr_avoided  / 0.89218
base_catch_gdf['tp_load_avoid']  = proj_prot_loads_comid_df.parcel_tpload_lbyr_avoided  / 0.89218
base_catch_gdf['tss_load_avoid'] = proj_prot_loads_comid_df.parcel_tssload_lbyr_avoided / 0.89218


# Fill NaN with 0, because arithmetric with a NaN produces NaN!
base_catch_gdf[['tn_load_avoid','tp_load_avoid','tss_load_avoid']] = base_catch_gdf[['tn_load_avoid','tp_load_avoid','tss_load_avoid']].fillna(0)


base_catch_gdf.info()


# Remaining Excess Concentration for Reaches
# NOTE: In Stage 1, we can not subtract non point source concentrations
base_reach_gdf['tn_conc_rem']  = base_reach_gdf.tn_conc_xs  - base_reach_gdf.tn_conc_red
base_reach_gdf['tp_conc_rem']  = base_reach_gdf.tp_conc_xs  - base_reach_gdf.tp_conc_red
base_reach_gdf['tss_conc_rem'] = base_reach_gdf.tss_conc_xs - base_reach_gdf.tss_conc_red


# Remaining Excess NPS Load Rates for Reaches
base_catch_gdf['tn_loadrate_rem']  = base_catch_gdf.tn_loadrate_xsnps - base_catch_gdf.tn_loadrate_red
base_catch_gdf['tp_loadrate_rem']  = base_catch_gdf.tp_loadrate_xsnps - base_catch_gdf.tp_loadrate_red
base_catch_gdf['tss_loadrate_rem'] = base_catch_gdf.tss_loadrate_xs   - base_catch_gdf.tss_loadrate_red


# Remaining Excess NPS Pollution in DRB, after Restoration
for pollutant in ['tn', 'tp', 'tss']:
    PlotMaps(base_reach_gdf, base_catch_gdf, 
             '%s_conc_rem' % pollutant, '%s_loadrate_rem' % pollutant, 
             targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
             cl=None, fa=False, zoom=False, diff=True)


for cluster in cluster_gdf.index.categories:
    print(cluster)


get_ipython().run_cell_magic("time", "", """# Remaining Excess NPS Pollution in a Cluster
for cluster in cluster_gdf.index.categories:
    for pollutant in ['tn', 'tp', 'tss']:
        PlotMaps(base_reach_gdf, base_catch_gdf, 
                 '%s_conc_rem' % pollutant, '%s_loadrate_rem' % pollutant, 
                 targets['%s' % pollutant]['conc_target'], targets['%s' % pollutant]['loadrate_target'],
                 cl=cluster, fa=True, zoom=True, diff=True)""")


# Proportion Remaining of Excess NPS Concentration for Reaches
# NOTE: In Stage 1, we can not subtract non point source concentrations
base_reach_gdf['tn_conc_rem_p']  = base_reach_gdf.tn_conc_rem / base_reach_gdf.tn_conc_xs
base_reach_gdf['tp_conc_rem_p']  = base_reach_gdf.tp_conc_rem / base_reach_gdf.tp_conc_xs
base_reach_gdf['tss_conc_rem_p'] = base_reach_gdf.tss_conc_rem / base_reach_gdf.tss_conc_xs


# Proportion Remaining of Excess NPS Load Rates for Catchments
base_catch_gdf['tn_loadrate_rem_p']  = base_catch_gdf.tn_loadrate_rem / base_catch_gdf.tn_loadrate_xsnps
base_catch_gdf['tp_loadrate_rem_p']  = base_catch_gdf.tp_loadrate_rem / base_catch_gdf.tp_loadrate_xsnps
base_catch_gdf['tss_loadrate_rem_p'] = base_catch_gdf.tss_loadrate_rem / base_catch_gdf.tss_loadrate_xs  # TSS has no point sources


mask_reach = base_reach_gdf['tp_conc_xs'] < 0
mask_reach.value_counts()


# Set proportion = 0 where Excess < 0
for pollutant in ['tn', 'tp', 'tss']:
    mask_reach = base_reach_gdf['%s_conc_xs' % pollutant] < 0
    base_reach_gdf.loc[mask_reach, '%s_conc_rem_p' % pollutant] = 0
    
    if pollutant != 'tss':
        mask_catch = base_catch_gdf['%s_loadrate_xsnps' % pollutant] < 0
    else:
        mask_catch = base_catch_gdf['%s_loadrate_xs' % pollutant] < 0
    base_catch_gdf.loc[mask_catch, '%s_loadrate_rem_p' % pollutant] = 0
       


base_reach_gdf[base_reach_gdf.fa_name=='White Lake Area']


base_catch_gdf[base_catch_gdf.fa_name=='White Lake Area']


base_catch_gdf[base_catch_gdf.cluster=='New Jersey Highlands'][base_catch_gdf.tp_loadrate_red>0].iloc[:,19:]


focusarea_gdf[focusarea_gdf.cluster=='New Jersey Highlands']


# Back calculate Loads (kg/y) from excess & remaining loading rates (kg/ha/y)
for pollutant in ['tn', 'tp', 'tss']:
    # Excess NPS Loads (_xsnps)
    if pollutant != 'tss':
        base_catch_gdf['%s_load_xsnps' % pollutant] = base_catch_gdf['%s_loadrate_xsnps' % pollutant] \
                                                      * base_catch_gdf.catchment_hectares
    else:  # TSS has no point sources, but let's label it as _xsnps for consistency
        base_catch_gdf['%s_load_xsnps' % pollutant] = base_catch_gdf['%s_loadrate_xs' % pollutant] \
                                                      * base_catch_gdf.catchment_hectares

    # Remaining Loads (_rem)
    base_catch_gdf['%s_load_rem' % pollutant] = base_catch_gdf['%s_loadrate_rem' % pollutant] \
                                                * base_catch_gdf.catchment_hectares


base_catch_gdf.head(3)


# temp_df = pd.read_excel(data_folder /'comids0204_in_drb.xlsx')
temp_df = pd.read_csv(data_folder /'comids0204_in_drb.csv',
                      index_col='comid',
                      dtype={'in_drb': 'category',
                             'huc08': 'category',
                            }
                     )
temp_df.info()


get_ipython().run_cell_magic("time", "", """temp_df.in_drb.value_counts()""")


# Add columns to df
base_catch_gdf[['in_drb','huc08']] = temp_df[['in_drb','huc08']]


base_catch_gdf.in_drb.value_counts()


drwi_load_df = base_catch_gdf.iloc[:, [0,1,2,3,22,23,35,37,39,36,38,40,41,42,43]].sum()
drwi_load_df


base_catch_gdf['cluster'].value_counts(dropna=False)


# Develop mask
mask = base_catch_gdf['cluster'].isnull()
mask.value_counts()


# Sum loads for DRWI, excluding Clusters via mask
mask = base_catch_gdf['cluster'].isnull()

# Preselect colums to keep
# Non-summable dtypes (object, category, geometry) will be dropped automatically
drwi_load_no_df = base_catch_gdf[mask].iloc[:, [3,        # catcment area
                                                    0,1,2,    # baseline loads
                                                    10,11,15, # cluster, fa, huc12 categories. Will be dropped if not used for grouping, because not summable.
                                                    22,23,    # point source loads
                                                    29,30,31, # avoided loads from land protection
                                                    38,40,42, # excess nonpoint source loads
                                                    39,41,43, # remaining loads after restoration
                                                    ]
                                                ].sum()

drwi_load_noClus_df


# Save to CSV file
drwi_load_noClus_df.to_csv(data_folder /'drwi_load_noClus.csv')


# Develop mask
mask = base_catch_gdf['in_drb'] == 't'
mask.value_counts()


# Sum loads for DRB, excluding Clusters via mask
mask = base_catch_gdf['in_drb'] == 't'

# Preselect colums to keep
# Non-summable dtypes (object, category, geometry) will be dropped automatically
drwi_load_drb_df = base_catch_gdf[mask].iloc[:, [3,        # catcment area
                                                    0,1,2,    # baseline loads
                                                    10,11,15, # cluster, fa, huc12 categories. Will be dropped if not used for grouping, because not summable.
                                                    22,23,    # point source loads
                                                    29,30,31, # avoided loads from land protection
                                                    38,40,42, # excess nonpoint source loads
                                                    39,41,43, # remaining loads after restoration
                                                    ]
                                                ].sum()

# drwi_load_drb_df


# Save to CSV file
drwi_load_drb_df.to_csv(data_folder /'drwi_load_drb.csv')


base_catch_gdf.cluster.value_counts()


# Sum loads by Cluster categories
# Preselect colums to keep
# Non-summable dtypes (object, category, geometry) will be dropped automatically

cluster_load_df = base_catch_gdf.iloc[:, [3,        # catcment area
                                          0,1,2,    # baseline loads
                                          10,11,15, # cluster, fa, huc12 categories. Will be dropped if not used for grouping, because not summable.
                                          22,23,    # point source loads
                                          29,30,31, # avoided loads from land protection
                                          38,40,42, # excess nonpoint source loads
                                          39,41,43, # remaining loads after restoration
                                         ]
                                     ].groupby('cluster').sum()

cluster_load_df.info()


cluster_load_df


# Save to CSV file
cluster_load_df.to_csv(data_folder /'cluster_loads.csv')


# merge or join with this?
cluster_gdf.info()


base_catch_gdf.fa_name.value_counts()


# Sum loads by Focus Area categories
# Preselect colums to keep
# Non-summable dtypes (object, category, geometry) will be dropped automatically

focusarea_load_df = base_catch_gdf.iloc[:, [3,        # catcment area
                                            0,1,2,    # baseline loads
                                            10,11,15, # cluster, fa, huc12 categories. Will be dropped if not used for grouping, because not summable.
                                            22,23,    # point source loads
                                            29,30,31, # avoided loads from land protection
                                            38,40,42, # excess nonpoint source loads
                                            39,41,43, # remaining loads after restoration
                                           ]
                                       ].groupby('fa_name').sum()

focusarea_load_df.info()


focusarea_load_df


# Add cluster, huc12, geom from `focusarea_gdf`
focusarea_load_df['cluster']        = focusarea_gdf['cluster']
focusarea_load_df['huc12']          = focusarea_gdf['huc12']
focusarea_load_df['geom_catchment'] = focusarea_gdf['geom_catchment']


focusarea_load_df.info()


focusarea_load_df.plot()


# Save to CSV file, excluding geometries.
focusarea_load_df.iloc[:,0:17].sort_values('cluster').to_csv(data_folder /'focusarea_loads.csv')


base_catch_gdf['fa_name'].value_counts(dropna=False)


# Develop mask
mask = base_catch_gdf['fa_name'].isnull()
mask.value_counts()


# Sum loads by Cluster categories, excluding Focus Areas via mask
mask = base_catch_gdf['fa_name'].isnull()

# Preselect colums to keep
# Non-summable dtypes (object, category, geometry) will be dropped automatically
cluster_load_noFA_df = base_catch_gdf[mask].iloc[:, [3,        # catcment area
                                                    0,1,2,    # baseline loads
                                                    10,11,15, # cluster, fa, huc12 categories. Will be dropped if not used for grouping, because not summable.
                                                    22,23,    # point source loads
                                                    29,30,31, # avoided loads from land protection
                                                    38,40,42, # excess nonpoint source loads
                                                    39,41,43, # remaining loads after restoration
                                                    ]
                                                ].groupby('cluster').sum()

cluster_load_noFA_df.info()


cluster_load_noFA_df


# Save to CSV file
cluster_load_noFA_df.to_csv(data_folder /'cluster_load_noFA.csv')



